version: '3.8'

services:
  triton:
    image: nvcr.io/nvidia/tritonserver:25.01-py3
    container_name: sam2-triton-server
    ports:
      - "8000:8000"  # HTTP
      - "8001:8001"  # gRPC
      - "8002:8002"  # Metrics
    volumes:
      - ./model_repository:/models
    command: tritonserver --model-repository=/models --log-verbose=1
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    shm_size: '2gb'
    ulimits:
      memlock: -1
      stack: 67108864
    environment:
      - CUDA_VISIBLE_DEVICES=0
